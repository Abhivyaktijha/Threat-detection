{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90791,"databundleVersionId":10592855,"sourceType":"competition"},{"sourceId":10728842,"sourceType":"datasetVersion","datasetId":6651543}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd #data preprocessing,CSV files I/O(e.g pd.read_csv)\nimport numpy as np#linear algebra\nimport matplotlib.pyplot as plt# data visualisation\nimport seaborn as sns#statistical data visualisation\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):# used to transverse a directory tree\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#uploading the file \ntrain_df=pd.read_csv('/kaggle/input/System-Threat-Forecaster/train.csv')\ntest_df=pd.read_csv('/kaggle/input/System-Threat-Forecaster/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**GETTING BASIC INFORMATION ABOUT DATASET**","metadata":{}},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The training datasets consist of 100000 rows and 76 columns.","metadata":{}},{"cell_type":"code","source":"train_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EXPLORATORY DATA ANALYSIS**","metadata":{}},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> The training datasets contains 31 float features, 17 integer features and 28 object features> .","metadata":{}},{"cell_type":"code","source":"train_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#missing values \nmissing_values=train_df.isnull().sum().sum()\nmissing_values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> There are total 6533 missing values in training datasets","metadata":{}},{"cell_type":"markdown","source":"**SUMMARY STATISTICS**","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical=train_df.select_dtypes(include=['int64','float64'])\nnumerical","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n> There are total 48 numerical features \n> 1. IsBetaUser can be dropped  as it has only \"0\" everywhere\n> 2. ReaTimeProtectionState can also be ignored as it is highly skewed and lacks variability, making it not recommended for analysis\n> 3. IsPassiveModeEnabled can be ignored because this variable's distribution is highly skewed and uninformative, as most of the values are concentrated at 0.\n> 4. NumAntivirusProductsEnabled and HasTPM can be ignored as the majority of data is concentrated at 1.\n> 5. IsTouchEnabled, IsAlwaysOnAlwaysConnectedCapable, and IsPenCapable can be ignores as it is highly skewed because majority of datasets contains '0'.\n> 6. PlatoformType can be ignore as it lacks variablility and is highly skewed.\n> 7. AutoSamplesubmission can be dropped as it has '0' everywhere.\n> 8. SMode can be dropped as it has highly skewed similarly Enable LUA , FirewallEnabled ,ProcessorManufacturerID,HasOpticalDiskDrive,ISportableOS, ISflightEnabled can be ignored. \n","metadata":{}},{"cell_type":"code","source":"## Descriptive statistics for categorical feature##\ncategorical=train_df.describe(include=['object'])\ncategorical","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> 1. MachineID has 99,835 unique values which is not helpful for modeling\n> 2. ProductName has win8defender with 99.7% dominancy hence can be ignore.\n> 3. PlatformType has dominance of 'windows10' and hence can be ignored for model building as distribution is skewed.\n> 4. Processor has dominacy of 'x64'and has outlier  can be ignore for model building as distribution is skewed.\n> 5. OSVersion has dominancy of '10.0.0.0' so it can also be ignore for model building as distribution is skewed.\n> 6. OSGenuineState has dominancy of of IS_GENUINE so it can be ignore for model building as distribution is skewed.\n> 7. FlightRing can also be ignore as its lack in variability and is skewed.\n> 8. DeviceFamily can also be drpped as it has 2 unique values but frequency of one unique value is very high and with similar reason OSArchitechture ca also be dropped. \n","metadata":{}},{"cell_type":"code","source":"target_counts = train_df['target'].value_counts()\nplt.figure(figsize=(8, 8))\nplt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=90, colors=['orange', 'lightgreen'])\nplt.title('Distribution of Target Classes')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> This pie chart indicated that target variable is uniformly distributed.","metadata":{}},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_df['DateAS']=pd.to_datetime(train_df['DateAS'],errors='coerce')\ntrain_df['month_as']=train_df['DateAS'].dt.month\ntrain_df['month_as']\ntest_df['DateAS']=pd.to_datetime(test_df['DateAS'],errors='coerce')\ntest_df['month_as']=test_df['DateAS'].dt.month\ntest_df['month_as']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dropping the redundant columns**","metadata":{}},{"cell_type":"code","source":"# List of columns to drop based on analysis\ncolumns_to_drop = [\n    \"IsBetaUser\", \"RealTimeProtectionState\", \"IsPassiveModeEnabled\", \n    \"NumAntivirusProductsEnabled\", \"HasTpm\", \"IsTouchEnabled\", \n    \"IsAlwaysOnAlwaysConnectedCapable\", \"IsPenCapable\", \"PlatformType\",\n     \"SMode\", \"EnableLUA\", \"FirewallEnabled\", \n    \"ProcessorManufacturerID\", \"HasOpticalDiskDrive\", \"IsPortableOS\", \n     \"MachineID\", \"ProductName\", \"PlatformType\", \n    \"Processor\", \"OSVersion\", \"OSGenuineState\", \"FlightRing\", \n    \"DeviceFamily\", \"OSArchitecture\"\n]\n\n# Drop the specified columns\ntrain_df_cleaned = train_df.drop(columns=columns_to_drop, axis=1)\ntest_df_cleaned=test_df.drop(columns=columns_to_drop, axis=1)\n\n# Display the shape of the dataset before and after dropping columns\nprint(\"Original dataset shape:\", train_df.shape)\nprint(\"Cleaned dataset shape:\", train_df_cleaned.shape)\nprint(\"Original dataset shape:\", test_df.shape)\nprint(\"Cleaned dataset shape:\", test_df_cleaned.shape)\ntrain_df_cleaned.to_csv(\"cleaned_train.csv\", index=False)\n# Display the first few rows of the cleaned dataset\ntrain_df_cleaned.head()\ntrain_df=train_df_cleaned\ntest_df=test_df_cleaned\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Calculating missing values percentage and plotting missing values via matplotlib library**","metadata":{}},{"cell_type":"code","source":"missing_values_train=train_df.isnull().sum().sum()\nmissing_values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(missing_values[missing_values > 0])\n\n# Missing values percentage\nmissing_values = train_df.isnull().sum() / len(train_df) * 100\nmissing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n\n# Plot missing values\nplt.figure(figsize=(12, 6))\nsns.barplot(x=missing_values.index, y=missing_values.values, palette=\"coolwarm\")\nplt.xticks(rotation=90)\nplt.ylabel(\"Missing Value Percentage\")\nplt.title(\"Missing Values in Training Data\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_col=['CityID','IsGamer','RegionIdentifier','InternalBatteryNumberOfCharges']\ntrain_df_hm=train_df.drop(columns=drop_col, axis=1)\ntrain_df_hm.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation heatmap for numerical features\nplt.figure(figsize=(12, 8))\ncorr_matrix = train_df_hm.select_dtypes(include=['float64', 'int64']).corr()\nsns.heatmap(corr_matrix, cmap='coolwarm', annot=False, linewidths=0.5)\nplt.title(\"Correlation Heatmap of Numerical Features\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> From the heatmap it can be say that :\n> 1.OSBuildNumber and OSBuildNumberOnly shows highly correaltion hence one of them can be dropped\n> PrimaryDisplayResolutionHorizontal and PrimaryDisplayResolutionHorizontal are also shows same property hence one of them can be dropped\n> PrimaryDiskCapacityMB and SystemVolumeCapacityMB are highly correlated so one can be dropped","metadata":{}},{"cell_type":"code","source":"# List of columns to drop based on high correlation\ncol_to_drop = [\n    \"OSBuildNumberOnly\",  # Dropping one of the highly correlated OS build features\n    \"PrimaryDisplayResolutionHorizontal\"  ,# Dropping one of the highly correlated resolution features\n    \"PrimaryDiskCapacityMB\" #Dropping highly correlated memory feature\n]\n# Drop the specified columns\ndf_cleaned = train_df.drop(columns=col_to_drop, axis=1)\ndf_cleaned_test = test_df.drop(columns=col_to_drop, axis=1)\n# Display the shape of the dataset before and after dropping columns\nprint(\"Original dataset shape:\", train_df.shape)\nprint(\"Cleaned dataset shape:\", df_cleaned.shape)\nprint(\"Original dataset shape:\", test_df.shape)\nprint(\"Cleaned dataset shape:\", df_cleaned_test.shape)\ndf_cleaned.columns\ntrain_df=df_cleaned\ntest_df=df_cleaned_test\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Date Time","metadata":{}},{"cell_type":"markdown","source":"**Univariate Analysis**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"columns = ['NumAntivirusProductsInstalled','IsSystemProtected','OSInstallLanguageID',\n          'OSBuildNumber', 'IsSecureBootEnabled', 'IsGamer']\n# Create a 2-row, 3-column subplot grid\nfig, axes = plt.subplots(2, 3, figsize=(15, 12))  # Corrected subplot initialization\n# Flatten axes array for easy iteration\naxes = axes.flatten()\n# Iterate over each feature and plot\nfor i, col in enumerate(columns):  # Corrected loop structure\n    sns.countplot(data=train_df, x=col, ax=axes[i])  # Plot countplot for each feature\n    axes[i].set_title(f'Distribution of {col}')  # Set title\n    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)  # Rotate x labels for readability\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_cat = ['EngineVersion', 'AppVersion', 'SignatureVersion',\n       'OsPlatformSubRelease', 'OSBuildLab', 'SKUEditionName']\n# Create a 2-row, 3-column subplot grid\nfig, axes = plt.subplots(2, 3, figsize=(15, 12))  # Corrected subplot initialization\n# Flatten axes array for easy iteration\naxes = axes.flatten()\n# Iterate over each feature and plot\nfor i, col in enumerate(columns_cat):  # Corrected loop structure\n    sns.countplot(data=train_df, x=col, ax=axes[i])  # Plot countplot for each feature\n    axes[i].set_title(f'Distribution of {col}')  # Set title\n    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)  # Rotate x labels for readability\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Multivariate analysis**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.countplot(data=train_df,x='IsGamer',hue='target')\nplt.title('Target Vs IsGamer Distribution')\nplt.xlabel('IsGamer')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Target')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.countplot(data=train_df,x='OSBuildLab',hue='target')\nplt.title('Target VsOSBuildLab Distribution')\nplt.xlabel('IsGamer')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Target')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature Extraction**","metadata":{}},{"cell_type":"code","source":"#Extracting month from DateAs and DateOS feature\ntrain_df['DateAS']=pd.to_datetime(train_df['DateAS'],errors='coerce')\ntrain_df['month_as']=train_df['DateAS'].dt.month\ntrain_df['month_as']\ntrain_df['DateOS']=pd.to_datetime(train_df['DateOS'],errors='coerce')\ntrain_df['month_os']=train_df['DateOS'].dt.month\ntrain_df['month_os']\ntest_df['DateAS']=pd.to_datetime(test_df['DateAS'],errors='coerce')\ntest_df['month_as']=test_df['DateAS'].dt.month\ntest_df['month_as']\ntest_df['DateOS']=pd.to_datetime(test_df['DateOS'],errors='coerce')\ntest_df['month_os']=test_df['DateOS'].dt.month\ntest_df['month_os']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dropping DateAs and DateOS feature after adding new month_as and month_os feature for training and test dta.\ndrop=['DateAS','DateOS']\ntrain_df=train_df.drop(columns=drop, axis=1)\nprint(train_df.shape)\ndrop=['DateAS','DateOS']\ntest_df=test_df.drop(columns=drop, axis=1)\nprint(test_df.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Splitting the data into train and validation set**","metadata":{}},{"cell_type":"code","source":"features=train_df.drop(columns='target')\nlabels=train_df['target']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(features,labels,test_size=0.2,random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****Spilting the X into numerical and categorical features  ****","metadata":{}},{"cell_type":"code","source":"categorical=features.select_dtypes(include=['object']).columns\nnumerical=features.select_dtypes(include=['int','float64']).columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Finding unique values in categorical feature","metadata":{}},{"cell_type":"code","source":"for col in categorical:\n    print(col,features[col].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Preprocessing the data**","metadata":{}},{"cell_type":"code","source":"cat_pipe=Pipeline(steps=[\n('imputation',SimpleImputer(strategy='most_frequent')),\n    ('encoding',OneHotEncoder(sparse=False,handle_unknown='ignore'))\n])\ncat_pipe","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ","metadata":{}},{"cell_type":"markdown","source":"> This categorical pipeline (cat_pipe) first fills missing values with the most frequent category using SimpleImputer, then converts categorical variables into one-hot encoded vectors using OneHotEncoder. The handle_unknown='ignore' ensures unseen categories in test data don’t cause errors. 🚀","metadata":{}},{"cell_type":"code","source":"num_pipe=Pipeline(steps=[\n('imputation',SimpleImputer(strategy='mean')),\n    ('scaling',StandardScaler())\n])\nnum_pipe","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> This numerical pipeline (num_pipe) first fills missing values with the mean using SimpleImputer, then standardizes numerical features to have zero mean and unit variance using StandardScaler. This helps improve model performance by ensuring features are on the same scale. ","metadata":{}},{"cell_type":"code","source":"transformer=ColumnTransformer(transformers=[\n    ('cat',cat_pipe, categorical),\n    ('num',num_pipe,numerical)\n])\ntransformer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":">This ColumnTransformer (transformer) applies cat_pipe to categorical features and num_pipe to numerical features, ensuring separate preprocessing for each type. It efficiently transforms the dataset by handling categorical encoding and numerical scaling in a single step.  ","metadata":{}},{"cell_type":"code","source":"X_train_processed=transformer.fit_transform(X_train)\nX_val_processed=transformer.transform(X_val)\ntest_processed=transformer.transform(test_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> These transformations ensure consistent preprocessing across datasets, with fit_transform(X_train) learning imputation, encoding, and scaling, while transform(X_val) and transform(test_df) apply the same transformations without data leakage. This keeps the model's input features standardized. ","metadata":{}},{"cell_type":"code","source":"X_train=pd.DataFrame(X_train_processed)\nX_val=pd.DataFrame(X_val_processed)\ntest=pd.DataFrame(test_processed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> These lines convert the transformed NumPy arrays back into pandas DataFrames, making it easier to analyze and use them in modeling.","metadata":{}},{"cell_type":"markdown","source":"**Model Building**","metadata":{}},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"model=LogisticRegression(random_state=42)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_val)\nscore=accuracy_score(y_val,y_pred)\nscore","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> This code trains a Logistic Regression model on the processed training data and evaluates it on the validation set using accuracy score.","metadata":{}},{"cell_type":"markdown","source":"**XGBOOST Classifier**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier \nmodel_xgb=XGBClassifier()\nmodel_xgb.fit(X_train,y_train)\ny_pred=model_xgb.predict(X_val)\nscore=accuracy_score(y_val,y_pred)\nscore\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Random Forest Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel_rfc= RandomForestClassifier()\nmodel_rfc.fit(X_train,y_train)\ny_pred=model_rfc.predict(X_val)\nscore=accuracy_score(y_val,y_pred)\nscore","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.svm import SVC\n# svm_model=SVC(kernel='rbf',C=1.0,gamma='scale')\n# svm_model.fit(X_train,y_train)\n# y_pred=svm_model.predict(X_val)\n# score=accuracy_score(y_val,y_pred)\n# score\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.neighbours import KNeighborsClassifier\n# model_knn KNeighborsClassifier(n_neighbors=5)\n# model_knn.fit(X_train,y_train)\n# y_pred=model_knn.predict(X_val)\n# score=accuracy_score(y_val,y_pred)\n# score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature Selection**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKbest\nselector=SelectKBest(score_func=f_classif,k=100)\nX_train_selectkbest=selector.fit_transform(X_train,y_train)\nX_val_selectkbest=selector.transform(X_val)\ntest_selectkbest=slector.transform(test)\n\nprint(X_train_selectkbest)\nprint(X_val_selectkbest)\nprint(test_selectkbest)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nselector_sfm=selectFromModel(estimator='LogosticRegression',n_estimator=100,random_stae=42,thresold='mean')\nX_train_SFM=selector_sfm.transform(X_train,y_train)\nX_val_SFM=selector_sfm.transform(X_val)\ntest_SFM=slector_sfm.transform(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}